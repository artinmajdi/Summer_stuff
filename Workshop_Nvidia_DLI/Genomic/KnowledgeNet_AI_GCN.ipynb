{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KnowledgeNet AI: Semi-Supervised Learning Applied to Graph Networks\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Graph networks are one the best ways to understand the hidden relationships in complex big data and in essence, reveal the unknown unknowns. Several industries already utilize graph networks and now artificial intelligence (AI) algorithms are being applied to them. However, it is not an easy endeavor and most attempts have been use case specific. \n",
    "![title](KnNet2.png)\n",
    "Source: https://lendix.com/project/cogiway/\n",
    "\n",
    "### Graph and Network Theory\n",
    "\n",
    "The diagram below is one of the simplest examples of a graph. The points A, B, C, D and E are vertices and the lines between them are edges. The intersection at AD and BE is not considered to be a vertex because it does not meet the requirements for being classified as a meeting of two wires or a cross-roads. Each vertex can serve as an endpoint and is associated with a degree metric that represents the amount of edges that are connected to that endpoint. \n",
    "\n",
    "\n",
    "![title](box1.png)\n",
    "\n",
    "Graph edges have different meanings depending on the type of graph being discussed. The two major types of graphs are undirected and directed. An undirected graph is a collection of connected nodes (i.e. vertices) where all the edges are bidirectional. A directed graph is a collection of connected nodes (i.e., vertices) with edges that are directed from one vertex to another. \n",
    "\n",
    "Networks are a map of interactions (i.e., relationships). These interactions are represented by nodes and links (i.e., edges). Nodes that are bunched together are known as clusters, where cliques are clusters that have a high degree of interaction. <b>Both are sub-graphs embedded within the main graph. As seen in the network map below, there are several clusters and cliques, can you identify two of each category?</b>\n",
    "\n",
    "![title](network.png)\n",
    "\n",
    "Source: http://www.martingrandjean.ch/gephi-introduction/\n",
    "\n",
    "<u>Important Concepts </u>\n",
    "\n",
    "<b>Vertices</b> (also known as nodes) are fundamental to all graphs. Each vertex is given a name, which is also called a key. \n",
    "Along with a name, each vertex can also have supplementary information, which is sometimes referred to as a payload. \n",
    "<b>Edges</b> (also known as arcs) are also fundamental to all graphs. Relationships within graphs are depicted as two vertices that are connected by an edge. When the edges all point in the same direction, the graph is considered as a directed graph (i.e., digraph). Edges can point in the same direction or in both directions. Graphs that have edges which are bidirectional are known as undirected graphs. Graph networks can contain edges that are weighted. These <b>weights</b> are often referred to as a cost and non-negative integers. Additionally, these weights can represent a variety of metrics such as distance between nodes, strength of relationship, etc. A <b>cycle</b> in a directed graph is a path that starts and ends at the same vertex.\n",
    "A <b>path</b> in a graph is a sequence of vertices that are connected by edges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Classification\n",
    "\n",
    "Last year, Thomas N. Kipf and Max Welling from the University of Amsterdam authored a conference paper that described an approach for applying AI to a knowledge graph. This approach is known as semi-supervised classification with graph convolutional networks.\n",
    "\n",
    "![title](cnn_graph.png)\n",
    "Source: http://tkipf.github.io/graph-convolutional-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Kipf and Welling 2017, there have been several attempts at applying semi-supervised learning to graph networks, and the majority of them fall into two major categories: Graph embedding based approaches and a variation of explicit graph Laplacian regularization. \n",
    "\n",
    "\n",
    "\n",
    "### Preparing a data set for the Graph Convolutional Neural Network Exercise\n",
    "\n",
    "First reformat the dataset to match the following styles:\n",
    "- an N by N adjacency matrix (N is the number of nodes)\n",
    "- an N by D feature matrix (D is the number of features per node)\n",
    "- an N by E binary label matrix (E is the number of classes)\n",
    "\n",
    "<b>Adjacency matrix</b><br>\n",
    "Is used to represent the weights of edges from vertex A to vertex B. For small graph data sets, the adjency matrix is sparse, because most of the cells are empty. One of the advantages of the adjacency matrix is that it allows users to easily view which nodes are connected to each other. The below images depict a graph network and its equivalent sparse adjacency matrix. In the sparse adjacency matrix, below there are multiple nodes. <b>Can you find the total number of cycles in the graph below?</b>\n",
    "![title](ad1.png)\n",
    "\n",
    "<b>Feature matrix</b><br>\n",
    "A matrix that depcits the number of features per node. \n",
    "\n",
    "<b>Binary label matrix</b><br>\n",
    "A matrix that depicts the physical location of the nodes, where a value of 1 represents the presence of a node and a value of 0 represents no node.\n",
    "\n",
    "\n",
    "<b>Biological Applications</b><br>\n",
    "Biological processes have been studied since the time of Aristotle, the Ancient Greek philosopher. The majority of biological research today is centered around the following areas: genomics, pharmacological sciences, diseases and biological network analysis. There are research projects that seek to amass and integrate publically available databases such as Reactome, KEGG, 1000 Genomes, TCGA, just to name a few, all for the purpose of finding hidden interactions and links between these types of information. The purpose of applying AI algorithms to these knowledge graphs is to derive hidden inisght from them, and thus help elucidate important biological functions that can help treat diseases.\n",
    "\n",
    "![title](bio3.png)\n",
    "\n",
    "\n",
    "Source: http://bioinformaticsonline.com/blog/view/8798/list-of-gene-ontology-software-and-tools\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Convolutional Neural Networks\n",
    "\n",
    "NetworkX is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks. Below is the code from Kipf and Welling's Graph Convolutional Neural Network demo.\n",
    "Modify the following values in the deep neural net architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In the file script.py please modufy the following code:\n",
    "\n",
    "## Insert SO dataset here, or three SO datasets\n",
    "flags.DEFINE_string('dataset', 'cora', 'Dataset string.')  # 'cora', 'citeseer', 'pubmed'\n",
    "## Choose a model\n",
    "flags.DEFINE_string('model', 'gcn', 'Model string.')  # 'gcn', 'gcn_cheby', 'dense'\n",
    "## Choose a learning rate\n",
    "flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\n",
    "## Choose any amount of epochs\n",
    "flags.DEFINE_integer('epochs', 200, 'Number of epochs to train.')\n",
    "## Choose any amount of nodes for the first Hidden layer \n",
    "flags.DEFINE_integer('hidden1', 16, 'Number of units in hidden layer 1.')\n",
    "## Choose any value for the dropout rate\n",
    "flags.DEFINE_float('dropout', 0.5, 'Dropout rate (1 - keep probability).')\n",
    "## Choose any value for the weight decay\n",
    "flags.DEFINE_float('weight_decay', 5e-4, 'Weight for L2 loss on embedding matrix.')\n",
    "## Choose any value for the early stopping\n",
    "flags.DEFINE_integer('early_stopping', 10, 'Tolerance for early stopping (# of epochs).')\n",
    "## Choose any value for the max degree\n",
    "flags.DEFINE_integer('max_degree', 3, 'Maximum Chebyshev polynomial degree.')\n",
    "\n",
    "## Then run the cell below to see the changes in accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-gpu\n",
      "  Downloading https://files.pythonhosted.org/packages/59/41/ba6ac9b63c5bfb90377784e29c4f4c478c74f53e020fa56237c939674f2d/tensorflow_gpu-1.8.0-cp36-cp36m-manylinux1_x86_64.whl (216.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 216.3MB 6.0kB/s eta 0:00:01   23% |███████▋                        | 51.3MB 71.0MB/s eta 0:00:03    51% |████████████████▋               | 112.2MB 52.7MB/s eta 0:00:02    62% |████████████████████            | 134.6MB 71.4MB/s eta 0:00:02    69% |██████████████████████▎         | 150.5MB 64.8MB/s eta 0:00:02    91% |█████████████████████████████▏  | 197.4MB 71.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow-gpu)\n",
      "Collecting tensorboard<1.9.0,>=1.8.0 (from tensorflow-gpu)\n",
      "  Downloading https://files.pythonhosted.org/packages/59/a6/0ae6092b7542cfedba6b2a1c9b8dceaf278238c39484f3ba03b03f07803c/tensorboard-1.8.0-py3-none-any.whl (3.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.1MB 450kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=0.1.6 (from tensorflow-gpu)\n",
      "  Downloading https://files.pythonhosted.org/packages/f7/69/6c86a51c6cf5ad6f09a54c9a7aae09850166af8d99cf1418e9ea9250baca/absl-py-0.2.1.tar.gz (81kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 12.0MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow-gpu)\n",
      "Collecting gast>=0.2.0 (from tensorflow-gpu)\n",
      "  Downloading https://files.pythonhosted.org/packages/5c/78/ff794fcae2ce8aa6323e789d1f8b3b7765f601e7702726f430e814822b96/gast-0.2.0.tar.gz\n",
      "Collecting grpcio>=1.8.6 (from tensorflow-gpu)\n",
      "  Downloading https://files.pythonhosted.org/packages/05/6d/f659683f0dfd3b3bebcd476821f2c598637e4e8948ff898f984867e96bc6/grpcio-1.12.0-cp36-cp36m-manylinux1_x86_64.whl (9.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 9.0MB 149kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor>=1.1.0 (from tensorflow-gpu)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting astor>=0.6.0 (from tensorflow-gpu)\n",
      "  Downloading https://files.pythonhosted.org/packages/b2/91/cc9805f1ff7b49f620136b3a7ca26f6a1be2ed424606804b0fbcf499f712/astor-0.6.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: wheel>=0.26 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow-gpu)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow-gpu)\n",
      "Collecting html5lib==0.9999999 (from tensorboard<1.9.0,>=1.8.0->tensorflow-gpu)\n",
      "  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\n",
      "\u001b[K    100% |████████████████████████████████| 890kB 1.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting bleach==1.5.0 (from tensorboard<1.9.0,>=1.8.0->tensorflow-gpu)\n",
      "  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow-gpu)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<1.9.0,>=1.8.0->tensorflow-gpu)\n",
      "  Downloading https://files.pythonhosted.org/packages/6d/7d/488b90f470b96531a3f5788cf12a93332f543dbab13c423a5e7ce96a0493/Markdown-2.6.11-py2.py3-none-any.whl (78kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 11.6MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from protobuf>=3.4.0->tensorflow-gpu)\n",
      "Building wheels for collected packages: absl-py, gast, termcolor, html5lib\n",
      "  Running setup.py bdist_wheel for absl-py ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/84/69/e0/6f4b789daf6cae7e70e5fda095603c7746cdc1a6013303c7ff\n",
      "  Running setup.py bdist_wheel for gast ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/9a/1f/0e/3cde98113222b853e98fc0a8e9924480a3e25f1b4008cedb4f\n",
      "  Running setup.py bdist_wheel for termcolor ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "  Running setup.py bdist_wheel for html5lib ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ec2-user/.cache/pip/wheels/50/ae/f9/d2b189788efcf61d1ee0e36045476735c838898eef1cad6e29\n",
      "Successfully built absl-py gast termcolor html5lib\n",
      "Installing collected packages: html5lib, bleach, markdown, tensorboard, absl-py, gast, grpcio, termcolor, astor, tensorflow-gpu\n",
      "  Found existing installation: html5lib 1.0.1\n",
      "    Uninstalling html5lib-1.0.1:\n",
      "      Successfully uninstalled html5lib-1.0.1\n",
      "  Found existing installation: bleach 2.1.2\n",
      "    Uninstalling bleach-2.1.2:\n",
      "      Successfully uninstalled bleach-2.1.2\n",
      "Successfully installed absl-py-0.2.1 astor-0.6.2 bleach-1.5.0 gast-0.2.0 grpcio-1.12.0 html5lib-0.9999999 markdown-2.6.11 tensorboard-1.8.0 tensorflow-gpu-1.8.0 termcolor-1.1.0\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/metrics.py:6: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "Epoch: 0001 train_loss= 1.10577 train_acc= 0.40000 val_loss= 1.10370 val_acc= 0.48200 time= 7.40430\n",
      "Epoch: 0002 train_loss= 1.09999 train_acc= 0.65000 val_loss= 1.10048 val_acc= 0.54400 time= 0.12127\n",
      "Epoch: 0003 train_loss= 1.09418 train_acc= 0.63333 val_loss= 1.09652 val_acc= 0.57600 time= 0.11820\n",
      "Epoch: 0004 train_loss= 1.08763 train_acc= 0.66667 val_loss= 1.09193 val_acc= 0.58600 time= 0.11885\n",
      "Epoch: 0005 train_loss= 1.07806 train_acc= 0.66667 val_loss= 1.08708 val_acc= 0.60200 time= 0.11858\n",
      "Epoch: 0006 train_loss= 1.07003 train_acc= 0.73333 val_loss= 1.08230 val_acc= 0.61800 time= 0.11657\n",
      "Epoch: 0007 train_loss= 1.06227 train_acc= 0.73333 val_loss= 1.07750 val_acc= 0.62800 time= 0.11915\n",
      "Epoch: 0008 train_loss= 1.05229 train_acc= 0.71667 val_loss= 1.07262 val_acc= 0.63400 time= 0.11862\n",
      "Epoch: 0009 train_loss= 1.04539 train_acc= 0.75000 val_loss= 1.06769 val_acc= 0.64400 time= 0.11993\n",
      "Epoch: 0010 train_loss= 1.03160 train_acc= 0.78333 val_loss= 1.06274 val_acc= 0.65200 time= 0.11828\n",
      "Epoch: 0011 train_loss= 1.02687 train_acc= 0.78333 val_loss= 1.05769 val_acc= 0.66000 time= 0.11682\n",
      "Epoch: 0012 train_loss= 1.01288 train_acc= 0.78333 val_loss= 1.05266 val_acc= 0.66200 time= 0.11824\n",
      "Epoch: 0013 train_loss= 1.00891 train_acc= 0.75000 val_loss= 1.04759 val_acc= 0.66800 time= 0.11758\n",
      "Epoch: 0014 train_loss= 0.99573 train_acc= 0.81667 val_loss= 1.04250 val_acc= 0.67800 time= 0.11687\n",
      "Epoch: 0015 train_loss= 0.98806 train_acc= 0.78333 val_loss= 1.03743 val_acc= 0.68200 time= 0.11923\n",
      "Epoch: 0016 train_loss= 0.97312 train_acc= 0.81667 val_loss= 1.03238 val_acc= 0.68400 time= 0.11714\n",
      "Epoch: 0017 train_loss= 0.96628 train_acc= 0.81667 val_loss= 1.02739 val_acc= 0.68600 time= 0.12069\n",
      "Epoch: 0018 train_loss= 0.95167 train_acc= 0.85000 val_loss= 1.02236 val_acc= 0.68800 time= 0.11678\n",
      "Epoch: 0019 train_loss= 0.94545 train_acc= 0.83333 val_loss= 1.01729 val_acc= 0.69000 time= 0.11671\n",
      "Epoch: 0020 train_loss= 0.91625 train_acc= 0.86667 val_loss= 1.01220 val_acc= 0.69200 time= 0.11707\n",
      "Epoch: 0021 train_loss= 0.92144 train_acc= 0.85000 val_loss= 1.00707 val_acc= 0.70200 time= 0.11688\n",
      "Epoch: 0022 train_loss= 0.91227 train_acc= 0.83333 val_loss= 1.00182 val_acc= 0.70200 time= 0.11565\n",
      "Epoch: 0023 train_loss= 0.90174 train_acc= 0.88333 val_loss= 0.99666 val_acc= 0.70600 time= 0.11765\n",
      "Epoch: 0024 train_loss= 0.88539 train_acc= 0.88333 val_loss= 0.99148 val_acc= 0.70800 time= 0.11654\n",
      "Epoch: 0025 train_loss= 0.85334 train_acc= 0.95000 val_loss= 0.98631 val_acc= 0.71400 time= 0.11604\n",
      "Epoch: 0026 train_loss= 0.85706 train_acc= 0.90000 val_loss= 0.98104 val_acc= 0.71600 time= 0.11622\n",
      "Epoch: 0027 train_loss= 0.86665 train_acc= 0.90000 val_loss= 0.97567 val_acc= 0.72000 time= 0.11564\n",
      "Epoch: 0028 train_loss= 0.85108 train_acc= 0.85000 val_loss= 0.97017 val_acc= 0.71800 time= 0.12093\n",
      "Epoch: 0029 train_loss= 0.82102 train_acc= 0.91667 val_loss= 0.96471 val_acc= 0.72400 time= 0.11666\n",
      "Epoch: 0030 train_loss= 0.82300 train_acc= 0.90000 val_loss= 0.95927 val_acc= 0.72600 time= 0.11594\n",
      "Epoch: 0031 train_loss= 0.82412 train_acc= 0.93333 val_loss= 0.95387 val_acc= 0.72600 time= 0.11738\n",
      "Epoch: 0032 train_loss= 0.80463 train_acc= 0.86667 val_loss= 0.94837 val_acc= 0.73600 time= 0.11705\n",
      "Epoch: 0033 train_loss= 0.78723 train_acc= 0.88333 val_loss= 0.94277 val_acc= 0.74200 time= 0.11625\n",
      "Epoch: 0034 train_loss= 0.78947 train_acc= 0.95000 val_loss= 0.93721 val_acc= 0.75000 time= 0.11661\n",
      "Epoch: 0035 train_loss= 0.74392 train_acc= 0.96667 val_loss= 0.93171 val_acc= 0.75000 time= 0.11615\n",
      "Epoch: 0036 train_loss= 0.75960 train_acc= 0.88333 val_loss= 0.92626 val_acc= 0.75400 time= 0.11619\n",
      "Epoch: 0037 train_loss= 0.74394 train_acc= 0.95000 val_loss= 0.92085 val_acc= 0.75400 time= 0.11664\n",
      "Epoch: 0038 train_loss= 0.76642 train_acc= 0.90000 val_loss= 0.91571 val_acc= 0.75600 time= 0.11706\n",
      "Epoch: 0039 train_loss= 0.73300 train_acc= 0.88333 val_loss= 0.91061 val_acc= 0.75800 time= 0.11675\n",
      "Epoch: 0040 train_loss= 0.72442 train_acc= 0.95000 val_loss= 0.90568 val_acc= 0.76000 time= 0.11649\n",
      "Epoch: 0041 train_loss= 0.69978 train_acc= 0.91667 val_loss= 0.90076 val_acc= 0.76400 time= 0.11631\n",
      "Epoch: 0042 train_loss= 0.71357 train_acc= 0.91667 val_loss= 0.89610 val_acc= 0.76600 time= 0.11574\n",
      "Epoch: 0043 train_loss= 0.68337 train_acc= 0.91667 val_loss= 0.89164 val_acc= 0.76600 time= 0.11656\n",
      "Epoch: 0044 train_loss= 0.67627 train_acc= 0.93333 val_loss= 0.88723 val_acc= 0.76600 time= 0.11614\n",
      "Epoch: 0045 train_loss= 0.64930 train_acc= 0.93333 val_loss= 0.88279 val_acc= 0.76600 time= 0.11629\n",
      "Epoch: 0046 train_loss= 0.65353 train_acc= 0.93333 val_loss= 0.87828 val_acc= 0.77200 time= 0.11720\n",
      "Epoch: 0047 train_loss= 0.63094 train_acc= 0.93333 val_loss= 0.87381 val_acc= 0.77200 time= 0.11634\n",
      "Epoch: 0048 train_loss= 0.65524 train_acc= 0.91667 val_loss= 0.86956 val_acc= 0.77400 time= 0.11587\n",
      "Epoch: 0049 train_loss= 0.61523 train_acc= 0.95000 val_loss= 0.86539 val_acc= 0.77400 time= 0.11678\n",
      "Epoch: 0050 train_loss= 0.62984 train_acc= 0.93333 val_loss= 0.86138 val_acc= 0.77600 time= 0.11611\n",
      "Epoch: 0051 train_loss= 0.65440 train_acc= 0.95000 val_loss= 0.85745 val_acc= 0.77600 time= 0.11578\n",
      "Epoch: 0052 train_loss= 0.65585 train_acc= 0.93333 val_loss= 0.85351 val_acc= 0.77400 time= 0.11515\n",
      "Epoch: 0053 train_loss= 0.54582 train_acc= 0.98333 val_loss= 0.84974 val_acc= 0.77600 time= 0.11531\n",
      "Epoch: 0054 train_loss= 0.60836 train_acc= 0.95000 val_loss= 0.84613 val_acc= 0.77800 time= 0.11770\n",
      "Epoch: 0055 train_loss= 0.58376 train_acc= 0.91667 val_loss= 0.84271 val_acc= 0.78400 time= 0.11731\n",
      "Epoch: 0056 train_loss= 0.58793 train_acc= 0.93333 val_loss= 0.83950 val_acc= 0.78800 time= 0.11679\n",
      "Epoch: 0057 train_loss= 0.56739 train_acc= 0.98333 val_loss= 0.83666 val_acc= 0.78800 time= 0.11583\n",
      "Epoch: 0058 train_loss= 0.54598 train_acc= 0.96667 val_loss= 0.83382 val_acc= 0.78400 time= 0.11950\n",
      "Epoch: 0059 train_loss= 0.57194 train_acc= 0.93333 val_loss= 0.83115 val_acc= 0.78200 time= 0.11707\n",
      "Epoch: 0060 train_loss= 0.58017 train_acc= 0.90000 val_loss= 0.82856 val_acc= 0.78600 time= 0.11663\n",
      "Epoch: 0061 train_loss= 0.56265 train_acc= 0.93333 val_loss= 0.82603 val_acc= 0.78600 time= 0.11688\n",
      "Epoch: 0062 train_loss= 0.58688 train_acc= 0.86667 val_loss= 0.82344 val_acc= 0.78600 time= 0.11601\n",
      "Epoch: 0063 train_loss= 0.53158 train_acc= 0.96667 val_loss= 0.82104 val_acc= 0.78600 time= 0.11674\n",
      "Epoch: 0064 train_loss= 0.55906 train_acc= 0.93333 val_loss= 0.81877 val_acc= 0.78600 time= 0.11553\n",
      "Epoch: 0065 train_loss= 0.52740 train_acc= 0.95000 val_loss= 0.81667 val_acc= 0.78800 time= 0.11570\n",
      "Epoch: 0066 train_loss= 0.52664 train_acc= 0.95000 val_loss= 0.81457 val_acc= 0.78600 time= 0.11644\n",
      "Epoch: 0067 train_loss= 0.49259 train_acc= 0.98333 val_loss= 0.81247 val_acc= 0.78400 time= 0.11669\n",
      "Epoch: 0068 train_loss= 0.49185 train_acc= 0.96667 val_loss= 0.81054 val_acc= 0.78400 time= 0.11757\n",
      "Epoch: 0069 train_loss= 0.55393 train_acc= 0.95000 val_loss= 0.80880 val_acc= 0.78600 time= 0.11691\n",
      "Epoch: 0070 train_loss= 0.49783 train_acc= 0.95000 val_loss= 0.80725 val_acc= 0.78200 time= 0.11775\n",
      "Epoch: 0071 train_loss= 0.49260 train_acc= 0.93333 val_loss= 0.80556 val_acc= 0.78200 time= 0.11664\n",
      "Epoch: 0072 train_loss= 0.48413 train_acc= 0.98333 val_loss= 0.80387 val_acc= 0.78000 time= 0.11725\n",
      "Epoch: 0073 train_loss= 0.48741 train_acc= 0.96667 val_loss= 0.80201 val_acc= 0.78000 time= 0.11672\n",
      "Epoch: 0074 train_loss= 0.49260 train_acc= 0.93333 val_loss= 0.80013 val_acc= 0.78000 time= 0.11653\n",
      "Epoch: 0075 train_loss= 0.49909 train_acc= 0.98333 val_loss= 0.79794 val_acc= 0.78000 time= 0.11678\n",
      "Epoch: 0076 train_loss= 0.49673 train_acc= 0.95000 val_loss= 0.79565 val_acc= 0.78200 time= 0.11884\n",
      "Epoch: 0077 train_loss= 0.48037 train_acc= 0.96667 val_loss= 0.79352 val_acc= 0.78200 time= 0.11633\n",
      "Epoch: 0078 train_loss= 0.48886 train_acc= 0.95000 val_loss= 0.79144 val_acc= 0.78200 time= 0.11774\n",
      "Epoch: 0079 train_loss= 0.49028 train_acc= 0.95000 val_loss= 0.78952 val_acc= 0.78200 time= 0.11693\n",
      "Epoch: 0080 train_loss= 0.46365 train_acc= 0.96667 val_loss= 0.78735 val_acc= 0.78400 time= 0.11676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0081 train_loss= 0.44647 train_acc= 0.96667 val_loss= 0.78506 val_acc= 0.78600 time= 0.11694\n",
      "Epoch: 0082 train_loss= 0.45919 train_acc= 0.96667 val_loss= 0.78280 val_acc= 0.78800 time= 0.11709\n",
      "Epoch: 0083 train_loss= 0.45923 train_acc= 0.98333 val_loss= 0.78064 val_acc= 0.78800 time= 0.11610\n",
      "Epoch: 0084 train_loss= 0.46924 train_acc= 0.98333 val_loss= 0.77870 val_acc= 0.78800 time= 0.11671\n",
      "Epoch: 0085 train_loss= 0.41709 train_acc= 0.96667 val_loss= 0.77691 val_acc= 0.78600 time= 0.11727\n",
      "Epoch: 0086 train_loss= 0.44221 train_acc= 0.98333 val_loss= 0.77515 val_acc= 0.78800 time= 0.11681\n",
      "Epoch: 0087 train_loss= 0.45529 train_acc= 0.95000 val_loss= 0.77359 val_acc= 0.79000 time= 0.11668\n",
      "Epoch: 0088 train_loss= 0.44594 train_acc= 0.95000 val_loss= 0.77188 val_acc= 0.79000 time= 0.11638\n",
      "Epoch: 0089 train_loss= 0.47604 train_acc= 0.95000 val_loss= 0.77085 val_acc= 0.78800 time= 0.11703\n",
      "Epoch: 0090 train_loss= 0.44703 train_acc= 0.98333 val_loss= 0.76968 val_acc= 0.78800 time= 0.11689\n",
      "Epoch: 0091 train_loss= 0.42752 train_acc= 0.98333 val_loss= 0.76829 val_acc= 0.78800 time= 0.11677\n",
      "Epoch: 0092 train_loss= 0.45924 train_acc= 0.98333 val_loss= 0.76704 val_acc= 0.78800 time= 0.11711\n",
      "Epoch: 0093 train_loss= 0.40568 train_acc= 0.96667 val_loss= 0.76588 val_acc= 0.78800 time= 0.11585\n",
      "Epoch: 0094 train_loss= 0.43822 train_acc= 0.96667 val_loss= 0.76479 val_acc= 0.78800 time= 0.11769\n",
      "Epoch: 0095 train_loss= 0.42721 train_acc= 0.96667 val_loss= 0.76397 val_acc= 0.78800 time= 0.11671\n",
      "Epoch: 0096 train_loss= 0.42830 train_acc= 0.95000 val_loss= 0.76314 val_acc= 0.78600 time= 0.11833\n",
      "Epoch: 0097 train_loss= 0.39985 train_acc= 0.98333 val_loss= 0.76271 val_acc= 0.78600 time= 0.11781\n",
      "Epoch: 0098 train_loss= 0.41658 train_acc= 0.98333 val_loss= 0.76200 val_acc= 0.78600 time= 0.11672\n",
      "Epoch: 0099 train_loss= 0.37416 train_acc= 0.96667 val_loss= 0.76140 val_acc= 0.78800 time= 0.11614\n",
      "Epoch: 0100 train_loss= 0.44039 train_acc= 0.96667 val_loss= 0.76062 val_acc= 0.78800 time= 0.11634\n",
      "Epoch: 0101 train_loss= 0.38206 train_acc= 0.98333 val_loss= 0.76004 val_acc= 0.78800 time= 0.11716\n",
      "Epoch: 0102 train_loss= 0.41820 train_acc= 0.98333 val_loss= 0.75925 val_acc= 0.79000 time= 0.11619\n",
      "Epoch: 0103 train_loss= 0.43043 train_acc= 0.95000 val_loss= 0.75903 val_acc= 0.79000 time= 0.11581\n",
      "Epoch: 0104 train_loss= 0.41965 train_acc= 0.98333 val_loss= 0.75861 val_acc= 0.79200 time= 0.11607\n",
      "Epoch: 0105 train_loss= 0.37669 train_acc= 0.98333 val_loss= 0.75831 val_acc= 0.79000 time= 0.11650\n",
      "Epoch: 0106 train_loss= 0.39785 train_acc= 0.98333 val_loss= 0.75765 val_acc= 0.79200 time= 0.11584\n",
      "Epoch: 0107 train_loss= 0.38541 train_acc= 0.98333 val_loss= 0.75680 val_acc= 0.79200 time= 0.11696\n",
      "Epoch: 0108 train_loss= 0.41473 train_acc= 0.96667 val_loss= 0.75576 val_acc= 0.79000 time= 0.11613\n",
      "Epoch: 0109 train_loss= 0.40745 train_acc= 0.98333 val_loss= 0.75440 val_acc= 0.78600 time= 0.11666\n",
      "Epoch: 0110 train_loss= 0.37027 train_acc= 0.98333 val_loss= 0.75310 val_acc= 0.78400 time= 0.11713\n",
      "Epoch: 0111 train_loss= 0.41579 train_acc= 0.95000 val_loss= 0.75178 val_acc= 0.78600 time= 0.11861\n",
      "Epoch: 0112 train_loss= 0.39360 train_acc= 0.98333 val_loss= 0.75024 val_acc= 0.78800 time= 0.11589\n",
      "Epoch: 0113 train_loss= 0.40174 train_acc= 1.00000 val_loss= 0.74886 val_acc= 0.79200 time= 0.11684\n",
      "Epoch: 0114 train_loss= 0.41291 train_acc= 0.98333 val_loss= 0.74722 val_acc= 0.79400 time= 0.11683\n",
      "Epoch: 0115 train_loss= 0.40107 train_acc= 0.96667 val_loss= 0.74582 val_acc= 0.79600 time= 0.11574\n",
      "Epoch: 0116 train_loss= 0.35862 train_acc= 0.98333 val_loss= 0.74434 val_acc= 0.79400 time= 0.11614\n",
      "Epoch: 0117 train_loss= 0.37642 train_acc= 0.98333 val_loss= 0.74283 val_acc= 0.79600 time= 0.11627\n",
      "Epoch: 0118 train_loss= 0.37836 train_acc= 0.98333 val_loss= 0.74167 val_acc= 0.79400 time= 0.11953\n",
      "Epoch: 0119 train_loss= 0.36178 train_acc= 0.98333 val_loss= 0.74083 val_acc= 0.79400 time= 0.11686\n",
      "Epoch: 0120 train_loss= 0.38550 train_acc= 0.98333 val_loss= 0.73994 val_acc= 0.78400 time= 0.11580\n",
      "Epoch: 0121 train_loss= 0.38423 train_acc= 0.98333 val_loss= 0.73964 val_acc= 0.78800 time= 0.11691\n",
      "Epoch: 0122 train_loss= 0.38162 train_acc= 0.98333 val_loss= 0.73921 val_acc= 0.79000 time= 0.11688\n",
      "Epoch: 0123 train_loss= 0.37810 train_acc= 1.00000 val_loss= 0.73876 val_acc= 0.78600 time= 0.11739\n",
      "Epoch: 0124 train_loss= 0.38356 train_acc= 0.98333 val_loss= 0.73862 val_acc= 0.78400 time= 0.11593\n",
      "Epoch: 0125 train_loss= 0.38269 train_acc= 0.96667 val_loss= 0.73899 val_acc= 0.78600 time= 0.11642\n",
      "Epoch: 0126 train_loss= 0.37011 train_acc= 1.00000 val_loss= 0.73930 val_acc= 0.78600 time= 0.11701\n",
      "Epoch: 0127 train_loss= 0.38556 train_acc= 0.98333 val_loss= 0.73933 val_acc= 0.78800 time= 0.11644\n",
      "Epoch: 0128 train_loss= 0.35798 train_acc= 1.00000 val_loss= 0.73929 val_acc= 0.78600 time= 0.11659\n",
      "Epoch: 0129 train_loss= 0.40202 train_acc= 0.98333 val_loss= 0.73871 val_acc= 0.78600 time= 0.11666\n",
      "Epoch: 0130 train_loss= 0.38212 train_acc= 0.96667 val_loss= 0.73818 val_acc= 0.78600 time= 0.11703\n",
      "Epoch: 0131 train_loss= 0.35286 train_acc= 1.00000 val_loss= 0.73720 val_acc= 0.78800 time= 0.11699\n",
      "Epoch: 0132 train_loss= 0.35251 train_acc= 0.98333 val_loss= 0.73561 val_acc= 0.78600 time= 0.11645\n",
      "Epoch: 0133 train_loss= 0.37164 train_acc= 0.96667 val_loss= 0.73384 val_acc= 0.78800 time= 0.11539\n",
      "Epoch: 0134 train_loss= 0.35523 train_acc= 0.95000 val_loss= 0.73269 val_acc= 0.78600 time= 0.11701\n",
      "Epoch: 0135 train_loss= 0.34741 train_acc= 0.98333 val_loss= 0.73119 val_acc= 0.78400 time= 0.11742\n",
      "Epoch: 0136 train_loss= 0.33145 train_acc= 0.98333 val_loss= 0.73015 val_acc= 0.78000 time= 0.11650\n",
      "Epoch: 0137 train_loss= 0.35452 train_acc= 0.98333 val_loss= 0.72953 val_acc= 0.78200 time= 0.11661\n",
      "Epoch: 0138 train_loss= 0.33129 train_acc= 0.98333 val_loss= 0.72933 val_acc= 0.78400 time= 0.11669\n",
      "Epoch: 0139 train_loss= 0.35409 train_acc= 0.98333 val_loss= 0.72909 val_acc= 0.78200 time= 0.11763\n",
      "Epoch: 0140 train_loss= 0.33767 train_acc= 0.98333 val_loss= 0.72918 val_acc= 0.78800 time= 0.11825\n",
      "Epoch: 0141 train_loss= 0.31017 train_acc= 0.98333 val_loss= 0.72935 val_acc= 0.78800 time= 0.11664\n",
      "Epoch: 0142 train_loss= 0.34474 train_acc= 0.98333 val_loss= 0.72954 val_acc= 0.78800 time= 0.11590\n",
      "Epoch: 0143 train_loss= 0.35739 train_acc= 0.98333 val_loss= 0.72942 val_acc= 0.78200 time= 0.11626\n",
      "Epoch: 0144 train_loss= 0.35909 train_acc= 0.98333 val_loss= 0.72971 val_acc= 0.77800 time= 0.11620\n",
      "Epoch: 0145 train_loss= 0.37009 train_acc= 0.96667 val_loss= 0.72966 val_acc= 0.77600 time= 0.11714\n",
      "Early stopping...\n",
      "Optimization Finished!\n",
      "Test set results: cost= 0.74244 accuracy= 0.79200 time= 0.06746\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install tensorflow-gpu\n",
    "\n",
    "%run script.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
