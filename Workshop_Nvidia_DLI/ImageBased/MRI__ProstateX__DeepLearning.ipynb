{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying MRI Images of Prostate Cancer with Mxnet Gluon\n",
    "\n",
    "Margaret Linan<br>\n",
    "Content Editing in Progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Prostate Cancer, MRI and Deep Learning\n",
    "\n",
    "According to the American Academy of Family Physicians, magnetic resonance imaging (MRI) is a healthcare technology that is used to survey patient organs and other anatomical structures with the goal of finding signs of disease, such as tumors and abnormalities. Currently, biomedical research aims to increase the resolution of MRI techology so that medical professionals can view tissue at the micrometer scale.\n",
    "\n",
    "The National Cancer Institute considers prostate cancer to be the most common cancer in men living in the United States. According to the American Cancer Society, there will be an estimated 161,360 new cases of prostate cancer in the United States in 2017, with 26,730 deaths.\n",
    "\n",
    "![title](Prostate_MRI.jpg)\n",
    "Source: https://www.alta-klinik.com/prostate-cancer.html\n",
    "\n",
    "Earlier diagnostic approaches such as Prostate Specific Antigen test and Digital Rectal examination, did not have good accuracy and may have caused an increase in false positive diagnoses. Although technological advances such as Magnetic Resonance Imagining (MRI) have resulted in increased accuracy for prostate cancer diagnoses, it's overall specificity needs improvement. Thus, insilico approaches such as machine learning and deep learning have been the focus of much work over the last decade. The challenges that exist with machine learning is the need to accurately identify multiple features in MRI images and the radiologists varying level of knowledge and skill in accurately identifying these. Deep learning algorithms such as Convolutional Neural Networks (CNN), give radiologists the opportunity to leverage digital pathology as a tool to accurately diagnose prostate cancer and reduce the number of unnecessary biopsies.\n",
    "\n",
    "### Deep Learning\n",
    "\n",
    "The advent of GPU technology makes it possible to apply deep learning neural network models to large MRI image collections. \n",
    "\n",
    "#### MRIs and Convolutional Neural Networks (MXNET - GLUON)\n",
    "\n",
    "Convolutional nets (CNN) are typically used to process data with a grid-like topology (1). Examples of this type of data include images, time-series, gene-expression (1,2). Examples of CNN applications include but are not limited to DNA sequence classification and modeling the sequence specificity of DNA-protein binding and biological relation extraction (3-5). \n",
    "The image below represents a CNN, in the feature learning stage convolutions are happening in parallel and linear activations are passed through the rectified linear activation (RELU) function (1). The pooling stages use a pooling function that then modifies the output of the layer (1). In the classification stage, new input is evaluated by the trained CNN and a classification prediction is made.\n",
    "\n",
    "![title](plot_nature2.jpg)\n",
    "Source: Nature Scientific Reports. doi:10.1038/s41598-017-15720-y\n",
    "\n",
    "## The Cancer Imaging Archive \n",
    "\n",
    "The Cancer Imaging Archive (TCIA), is described on its website as the following: TCIA is a service which de-identifies and hosts a large archive of medical images of cancer accessible for public download. The data are organized as “Collections”, typically patients related by a common disease (e.g. lung cancer), image modality (MRI, CT, etc) or research focus. DICOM is the primary file format used by TCIA for image storage. Supporting data related to the images such as patient outcomes, treatment details, genomics, pathology, and expert analyses are also provided when available.\n",
    "\n",
    "![title](image1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's begin the Deep Learning Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MXNET Convolutional Neural Networks Code\n",
    "## Source:  http://gluon.mxnet.io/chapter04_convolutional-neural-networks/cnn-gluon.html\n",
    "## Note: Must make the mxnet library\n",
    "import os, sys\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import glob\n",
    "from __future__ import print_function\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd, gluon\n",
    "mx.random.seed(1)\n",
    "\n",
    "## Set the context \n",
    "ctx = mx.gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-f1cf79cfbbe4>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-f1cf79cfbbe4>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    source: https://towardsdatascience.com/multi-label-image-classification-with-inception-net-cbb2ee538e30\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "### Alternative code for importing labels and images + predictions\n",
    "# source: https://github.com/BartyzalRadek/Multi-label-Inception-net/blob/master/label_image.py\n",
    "# source: https://towardsdatascience.com/multi-label-image-classification-with-inception-net-cbb2ee538e30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "-f; No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e8413afabd8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Read in the image_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mimage_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFastGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Loads label file, strips off carriage return\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    118\u001b[0m       \u001b[0mstring\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mregular\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \"\"\"\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preread_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36m_preread_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         self._read_buf = pywrap_tensorflow.CreateBufferedInputStream(\n\u001b[0;32m---> 80\u001b[0;31m             compat.as_bytes(self.__name), 1024 * 512, status)\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_prewrite_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    520\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: -f; No such file or directory"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import sys\n",
    "\n",
    "# change this as you see fit\n",
    "image_path = sys.argv[1]\n",
    "\n",
    "# Read in the image_data\n",
    "image_data = tf.gfile.FastGFile(image_path, 'rb').read()\n",
    "\n",
    "# Loads label file, strips off carriage return\n",
    "label_lines = [line.rstrip() for line \n",
    "                   in tf.gfile.GFile(\"labels.txt\")]\n",
    "\n",
    "# Unpersists graph from file\n",
    "with tf.gfile.FastGFile(\"retrained_graph.pb\", 'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "    _ = tf.import_graph_def(graph_def, name='')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Feed the image_data as input to the graph and get first prediction\n",
    "    softmax_tensor = sess.graph.get_tensor_by_name('final_result:0')\n",
    "    \n",
    "    predictions = sess.run(softmax_tensor, \\\n",
    "             {'DecodeJpeg/contents:0': image_data})\n",
    "    \n",
    "    # Sort to show labels of first prediction in order of confidence\n",
    "    top_k = predictions[0].argsort()[-len(predictions[0]):][::-1]\n",
    "    \n",
    "    for node_id in top_k:\n",
    "        human_string = label_lines[node_id]\n",
    "        score = predictions[0][node_id]\n",
    "        print('%s (score = %.5f)' % (human_string, score))\n",
    "    \n",
    "\n",
    "    filename = \"results.txt\"    \n",
    "    with open(filename, 'a+') as f:\n",
    "        f.write('\\n**%s**\\n' % (image_path))\n",
    "        for node_id in top_k:\n",
    "            human_string = label_lines[node_id]\n",
    "            score = predictions[0][node_id]\n",
    "f.write('%s (score = %.5f)\\n' % (human_string, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Specify the input and output\n",
    "The input for this CNN model will be the MRI images of the lesions from the prostate cancer patients.\n",
    "The output will be the labels, where \"1\" represents clinical significance and \"0\" represents no clinical significance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-52bd3fa04f1f>, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-52bd3fa04f1f>\"\u001b[0;36m, line \u001b[0;32m15\u001b[0m\n\u001b[0;31m    (/---------------------------------------------------------------------%)\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64  # number of samples per batch\n",
    "num_inputs = 231\n",
    "num_outputs = 99\n",
    "\n",
    "train_image_list = []\n",
    "for filename in glob.glob('yourpath/*.bmp'): \n",
    "    train_im=Image.open(filename)\n",
    "    train_image_list.append(train_im)\n",
    "\n",
    "test_image_list = []\n",
    "for filename in glob.glob('yourpath/*.bmp'): \n",
    "    test_im=Image.open(filename)\n",
    "    test_image_list.append(test_im)\n",
    " \n",
    "def transform(data, label):\n",
    "    return nd.transpose(data.astype(np.float32), (2,0,1))/255, label.astype(np.float32)\n",
    "\n",
    "\n",
    "train_data = gluon.data.DataLoader(train_im(train=True, transform=transform), batch_size, shuffle=True)\n",
    "test_data = gluon.data.DataLoader(test_im(train=False, transform=transform), batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network or Deep CNN\n",
    "\n",
    "Now let's define a convolutional neural network. According to the documentation in MXNET Gluon:<br> \n",
    "\n",
    "<b>num_fc</b> = specifies the number of neurons in the dense layer.<br>\n",
    "<b>num_outputs</b> = specifies the number of neurons in the<br> \n",
    "<b>Conv2D</b> = This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None, it is applied to the outputs as well.<br>\n",
    "<b>MaxPool2D</b> = Max pooling operation for two dimensional (spatial) data.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_fc = 512\n",
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Conv2D(channels=20, kernel_size=5, activation='relu'))\n",
    "    net.add(gluon.nn.MaxPool2D(pool_size=2, strides=2))\n",
    "    net.add(gluon.nn.Conv2D(channels=50, kernel_size=5, activation='relu'))\n",
    "    net.add(gluon.nn.MaxPool2D(pool_size=2, strides=2))\n",
    "## The Flatten layer collapses all axis, except the first one, into one axis.\n",
    "    net.add(gluon.nn.Flatten())\n",
    "    net.add(gluon.nn.Dense(num_fc, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(num_outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Parameter initialization\n",
    "In this situation, gluon is not actually initializing any parameters! Instead, it’s making a note of which \n",
    "initializer to associate with each parameter, even though its shape is not yet known. The parameters are \n",
    "instantiated and the initializer is called once we provide the network with some input.\n",
    "source: http://gluon.mxnet.io/chapter03_deep-neural-networks/plumbing.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Softmax Cross-Entropy Loss\n",
    "The Softmax classifier is one of the most popular classifiers in the deep learning field. \n",
    "Cross-entropy loss represents the distance or difference between what the model believes \n",
    "the output distribution should be, and what the original distribution really is. \n",
    "Note: equations, explanations and images: https://deepnotes.io/softmax-crossentropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gluon' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-eb8a1a9b7af3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msoftmax_cross_entropy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgluon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSoftmaxCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'gluon' is not defined"
     ]
    }
   ],
   "source": [
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Optimizer\n",
    "The purpose of the SGD optimizer is to find the minima or maxima by iteration, based on the learning rate.\n",
    "SGD is one of the most used optimization algorithms in the deep learning field. According to Goodfellow, Bengio, Courville 2016, the learning rate is critical and needed because SGD gradient estimator generates noise (the random sample of m training examples) that remain even when the minimum has been reached. Although the learning rate can be chosen by using the trial and error method, Goodfellow, Bengio, Courville 2016 offers the following advice on how to choose the best learning rate:\n",
    " \n",
    "  1. Monitor the the learning curves that plot the objective functions as a function of time\n",
    "  2. If the learning rate is too large, the oscillations will be violent, with the cost function often increasing significantly\n",
    "  3. Mild oscillations are more acceptable especially if you are training with a stochastic cost function such as the cost function \n",
    "     arising from the use of dropout (Monte Carlo dropout is suggested, see https://arxiv.org/pdf/1506.02158v6.pdf). \n",
    "  4. If the learning rate is too small, learning may be stuck with a high cost value\n",
    " \n",
    "Final thoughts offered by Goodfellow, Bengio, Courville 2016:\n",
    "  1. Typically, the optimal intial learning rate, in terms of total training time and the final cost value, is higher than the \n",
    "     learning rate that yields the best performance after the first 100 iterations or so\n",
    "  2. Do not use a learning rate that is so high that it causes severe instability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': .1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Write an evaluation loop to calculate the model's accuracy\n",
    "\n",
    "def evaluate_accuracy(data_iterator, net):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    return acc.get()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mxnet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7a8de73cc0d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mmxnet\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmxnet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mautograd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgluon\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mxnet'"
     ]
    }
   ],
   "source": [
    "## Training Loop\n",
    "## Epochs = 1,  means that there will be one full pass of the training data set. \n",
    "## Smoothing_constant = allows some control of the bias of the variance estimation,\n",
    "##                      and thus effectively smoothes the estimate.\n",
    "\n",
    "epochs = 1\n",
    "smoothing_constant = .01\n",
    "\n",
    "for e in range(epochs):\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "\n",
    "        ##########################\n",
    "        #  Keep a moving average of the losses\n",
    "        ##########################\n",
    "        curr_loss = nd.mean(loss).asscalar()\n",
    "        moving_loss = (curr_loss if ((i == 0) and (e == 0))\n",
    "                       else (1 - smoothing_constant) * moving_loss + smoothing_constant * curr_loss)\n",
    "\n",
    "    test_accuracy = evaluate_accuracy(test_data, net)\n",
    "    train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    \n",
    "## We can now evaulate the accuracy of the model   \n",
    "    \n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" % (e, moving_loss, train_accuracy, test_accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Precision-Recall Analysis\n",
    "\n",
    "The Precision-Recall metric is used to evaluate the classifiers output quality.\n",
    "The results from a precision-recall analysis indicate the classifiers success at accurately predicting the classes of \n",
    "the dataset.\n",
    " \n",
    "Precision measures the relevancy of the result, while recall measures the amount of truly relevant results. \n",
    "Visualizing the results of the precision-recall analysis helps communicate the tradeoff between precisian and recall for different thresholds. \n",
    "A large area under the curve (AUC) value inducates that there is a high recall and precision. \n",
    "\n",
    " High Precision ~ low false positive rate\n",
    " High Recall ~ low false negative rate \n",
    "\n",
    "When the precision analysis is able to prove that there is high precision and high recall for the classifier, it means \n",
    "that the classifier is accurate and is mostly returning all positive results. \n",
    "\n",
    "<b>General Expectations </b>\n",
    "\n",
    " - Classifiers with high recall but low precision returns many results, but most of its predicted labels are incorrect when  \n",
    "   compared to the training labels. \n",
    " - Classifiers with high precision but low recall is just the opposite, returning very few results, but most of its predicted \n",
    "   labels are correct when compared to the training labels. An ideal system with high precision and high recall will return \n",
    "   many results, with all results labeled correctly.\n",
    "\n",
    "Precision (P) is defined as the number of true positives (T_p) over the number of true positives plus the number of false positives (F_p).\n",
    "\n",
    " P = {T_p} / {T_p+F_p}\n",
    "\n",
    "Recall (R) is defined as the number of true positives (T_p) over the number of true positives plus the number of false negatives (F_n).\n",
    " \n",
    " R = {T_p} / {T_p + F_n}\n",
    " \n",
    "These quantities are also related to the (F_1) score, which is defined as the harmonic mean of precision and recall.\n",
    " \n",
    " F1 = 2*{P * R} / {P+R}\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-da530ab380d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcolors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcycle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'navy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'turquoise'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'darkorange'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cornflowerblue'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'teal'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'dimgray'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'darkviolet'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mf_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "## Code for visualizing the results of the precision-recall analysis\n",
    "## http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html\n",
    " \n",
    "from itertools import cycle\n",
    "# setup plot details, there are seven classes in the dataset, thus seven colors\n",
    "colors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal', 'dimgray', 'darkviolet'])\n",
    "\n",
    "plt.figure(figsize=(7, 8))\n",
    "f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "lines = []\n",
    "labels = []\n",
    "for f_score in f_scores:\n",
    "    x = np.linspace(0.01, 1)\n",
    "    y = f_score * x / (2 * x - f_score)\n",
    "    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "    plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "\n",
    "lines.append(l)\n",
    "labels.append('iso-f1 curves')\n",
    "l, = plt.plot(recall[\"micro\"], precision[\"micro\"], color='gold', lw=2)\n",
    "lines.append(l)\n",
    "labels.append('micro-average Precision-recall (area = {0:0.2f})'\n",
    "              ''.format(average_precision[\"micro\"]))\n",
    "\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    l, = plt.plot(recall[i], precision[i], color=color, lw=2)\n",
    "    lines.append(l)\n",
    "    labels.append('Precision-recall for class {0} (area = {1:0.2f})'\n",
    "                  ''.format(i, average_precision[i]))\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.subplots_adjust(bottom=0.25)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Extension of Precision-Recall curve to multi-class')\n",
    "plt.legend(lines, labels, loc=(0, -.38), prop=dict(size=14))\n",
    "\n",
    "\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other ideas for the lab\n",
    "https://hackernoon.com/transfer-learning-with-mxnet-gluon-8203005afafe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
